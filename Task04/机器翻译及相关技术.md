# 机器翻译及相关技术
**机器翻译和数据集**

机器翻译（MT）：将一段文本从一种语言自动翻译为另一种语言，用神经网络解决这个问题通常称为神经机器翻译（NMT）。 主要特征：输出是单词序列而不是单个单词。 输出序列的长度可能与源序列的长度不同。

In [28]:

    import os 
    os.listdir('/home/kesci/input/')

Out[28]:

    ['fraeng6506', 'd2l9528', 'd2l6239']

In [1]:

    import sys 
    sys.path.append('/home/kesci/input/d2l9528/') 
    import collections 
    import d2l 
    import zipfile from d2l.data.base 
    import Vocab import time 
    import torch 
    import torch.nn as nn 
    import torch.nn.functional as F 
    from torch.utils 
    import data from torch 
    import optim

**数据预处理**

将数据集清洗、转化为神经网络的输入minbatch

In [2]:

with open('/home/kesci/input/fraeng6506/fra.txt', 'r') as f:      
        raw_text = f.read() 
print(raw_text[0:1000])

Go.	Va !	CC-BY 2.0 (France) Attribution: tatoeba.org #2877272 (CM) & #1158250 (Wittydev)<br> 
Hi.	Salut !	CC-BY 2.0 (France) Attribution: tatoeba.org #538123 (CM) & #509819 (Aiji) <br>
Hi.	Salut.	CC-BY 2.0 (France) Attribution: tatoeba.org #538123 (CM) & #4320462 (gillux) <br>
Run!	Cours !	CC-BY 2.0 (France) Attribution: tatoeba.org #906328 (papabear) & #906331 (sacredceltic) <br>
Run!	Courez !	CC-BY 2.0 (France) Attribution: tatoeba.org #906328 (papabear) & #906332 (sacredceltic) <br>
Who?	Qui ?	CC-BY 2.0 (France) Attribution: tatoeba.org #2083030 (CK) & #4366796 (gillux) <br>
Wow!	Ça alors !	CC-BY 2.0 (France) Attribution: tatoeba.org #52027 (Zifre) & #374631 (zmoo) <br>
Fire!	Au feu !	CC-BY 2.0 (France) Attribution: tatoeba.org #1829639 (Spamster) & #4627939 <br>(sacredceltic) 
Help!	À l'aide !	CC-BY 2.0 (France) Attribution: tatoeba.org #435084 (lukaszpp) & #128430 (sysko) <br>
Jump.	Saute.	CC-BY 2.0 (France) Attribution: tatoeba.org #631038 (Shishir) & #2416938 (Phoenix) <br>
Stop!	Ça suffit !	CC-BY 2.0 (France) Attribution: tato <br>

In [3]:

    def preprocess_raw(text):    
        text = text.replace('\u202f', ' ').replace('\xa0', ' ')    
        out = ''    
        for i, char in enumerate(text.lower()):        
            if char in (',', '!', '.') and i > 0 and text[i-1] != ' ':            
                out += ' '        
            out += char    
        return out 

    text = preprocess_raw(raw_text) 
    print(text[0:1000])

go .	va !	cc-by 2 .0 (france) attribution: tatoeba .org #2877272 (cm) & #1158250 (wittydev) <br>
hi .	salut !	cc-by 2 .0 (france) attribution: tatoeba .org #538123 (cm) & #509819 (aiji) <br>
hi .	salut .	cc-by 2 .0 (france) attribution: tatoeba .org #538123 (cm) & #4320462 (gillux) <br>
run !	cours !	cc-by 2 .0 (france) attribution: tatoeba .org #906328 (papabear) & #906331 (sacredceltic) <br>
run !	courez !	cc-by 2 .0 (france) attribution: tatoeba .org #906328 (papabear) & #906332 (sacredceltic) <br>
who?	qui ?	cc-by 2 .0 (france) attribution: tatoeba .org #2083030 (ck) & #4366796 (gillux) <br>
wow !	ça alors !	cc-by 2 .0 (france) attribution: tatoeba .org #52027 (zifre) & #374631 (zmoo)<br> 
fire !	au feu !	cc-by 2 .0 (france) attribution: tatoeba .org #1829639 (spamster) & #4627939 (sacredceltic) <br>
help !	à l'aide !	cc-by 2 .0 (france) attribution: tatoeba .org #435084 (lukaszpp) & #128430 (sysko) <br>
jump .	saute .	cc-by 2 .0 (france) attribution: tatoeba .org #631038 (shishir) & #2416938 (phoenix)<br>
 stop !	ça suffit !	cc-b

字符在计算机里是以编码的形式存在，我们通常所用的空格是 \x20 ，是在标准ASCII可见字符 0x20~0x7e 范围内。 而 \xa0 属于 latin1 （ISO/IEC_8859-1）中的扩展字符集字符，代表不间断空白符nbsp(non-breaking space)，超出gbk编码范围，是需要去除的特殊字符。再数据预处理的过程中，我们首先需要对数据进行清洗。

**分词**

字符串---单词组成的列表

In [4]:

    num_examples = 50000 
    source, target = [], [] 
    for i, line in enumerate(text.split('\n')):    
        if i > num_examples:        
            break    
        parts = line.split('\t')    
        if len(parts) >= 2:        
            source.append(parts[0].split(' '))        
            target.append(parts[1].split(' '))         

    source[0:3], target[0:3]

Out[4]:

    ([['go', '.'], ['hi', '.'], ['hi', '.']], [['va', '!'], ['salut', '!'], ['salut', '.']])

In [5]:

    d2l.set_figsize() 
    d2l.plt.hist([[len(l) for l in source], [len(l) for l in target]],label=['source', 'target']) 
    d2l.plt.legend(loc='upper right');

![img](https://cdn.kesci.com/rt_upload/7589E7D345B3463A8F0F4574ED6EDA9A/q5jefa8ffq.svg)

**建立词典**

单词组成的列表---单词id组成的列表

In [6]:

    def build_vocab(tokens):    
        tokens = [token for line in tokens for token in line]    
        return d2l.data.base.Vocab(tokens, min_freq=3, use_special_tokens=True) 

    src_vocab = build_vocab(source) 
    len(src_vocab)

Out[6]:

    3789

![img](https://github.com/makeittrue/dssdxx-learning-note/blob/master/images/Task04/%E6%9C%BA%E5%99%A8%E7%BF%BB%E8%AF%91%E5%8F%8A%E7%9B%B8%E5%85%B3%E6%8A%80%E6%9C%AF/01.png)

**载入数据集**

In [7]:

    def pad(line, max_len, padding_token):    
        if len(line) > max_len:        
            return line[:max_len]    
        return line + [padding_token] * (max_len - len(line)) 
    pad(src_vocab[source[0]], 10, src_vocab.pad) 

Out[7]:

    [38, 4, 0, 0, 0, 0, 0, 0, 0, 0]

In [8]:

    def build_array(lines, vocab, max_len, is_source):    
        lines = [vocab[line] for line in lines]    
        if not is_source:        
            lines = [[vocab.bos] + line + [vocab.eos] for line in lines]    
        array = torch.tensor([pad(line, max_len, vocab.pad) for line in lines])    
        valid_len = (array != vocab.pad).sum(1) #第一个维度    
        return array, valid_len

![img](https://github.com/makeittrue/dssdxx-learning-note/blob/master/images/Task04/%E6%9C%BA%E5%99%A8%E7%BF%BB%E8%AF%91%E5%8F%8A%E7%9B%B8%E5%85%B3%E6%8A%80%E6%9C%AF/02.png)

In [9]:

    def load_data_nmt(batch_size, max_len): # This function is saved in d2l.    
        src_vocab, tgt_vocab = build_vocab(source), build_vocab(target)    
        src_array, src_valid_len = build_array(source, src_vocab, max_len, True)    
        tgt_array, tgt_valid_len = build_array(target, tgt_vocab, max_len, False)    
        train_data = data.TensorDataset(src_array, src_valid_len, tgt_array, tgt_valid_len)    
        train_iter = data.DataLoader(train_data, batch_size, shuffle=True)    
        return src_vocab, tgt_vocab, train_iter

In [10]:

    src_vocab, tgt_vocab, train_iter = load_data_nmt(batch_size=2, max_len=8) 
    for X, X_valid_len, Y, Y_valid_len, in train_iter:    
        print('X =', X.type(torch.int32), '\nValid lengths for X =', X_valid_len,        
            '\nY =', Y.type(torch.int32), '\nValid lengths for Y =', Y_valid_len)    
        break

X = tensor([[   5,   24,    3,    4,    0,    0,    0,    0],        [  12, 1388,    7,    3,    4,    0,    0,    0]], dtype=torch.int32)  
Valid lengths for X = tensor([4, 5])  
Y = tensor([[   1,   23,   46,    3,    3,    4,    2,    0],        [   1,   15,  137,   27, 4736,    4,    2,    0]], dtype=torch.int32)  
Valid lengths for Y = tensor([7, 7])

**Encoder-Decoder**

encoder：输入到隐藏状态

decoder：隐藏状态到输出

![img](https://github.com/makeittrue/dssdxx-learning-note/blob/master/images/Task04/%E6%9C%BA%E5%99%A8%E7%BF%BB%E8%AF%91%E5%8F%8A%E7%9B%B8%E5%85%B3%E6%8A%80%E6%9C%AF/03.png)

In [11]:

    class Encoder(nn.Module):    
        def __init__(self, **kwargs):        
            super(Encoder, self).__init__(**kwargs)     
        def forward(self, X, *args):        
            raise NotImplementedError

In [12]:

    class Decoder(nn.Module):    
        def __init__(self, **kwargs):        
            super(Decoder, self).__init__(**kwargs)     
        def init_state(self, enc_outputs, *args):        
            raise NotImplementedError     
        def forward(self, X, state):        
            raise NotImplementedError

In [13]:

    class EncoderDecoder(nn.Module):    
        def __init__(self, encoder, decoder, **kwargs):        
            super(EncoderDecoder, self).__init__(**kwargs)        
            self.encoder = encoder        
            self.decoder = decoder     
        def forward(self, enc_X, dec_X, *args):        
            enc_outputs = self.encoder(enc_X, *args)        
            dec_state = self.decoder.init_state(enc_outputs, *args)        
            return self.decoder(dec_X, dec_state)

可以应用在对话系统、生成式任务中。

**Sequence to Sequence模型**

**模型：**

训练

![img](https://github.com/makeittrue/dssdxx-learning-note/blob/master/images/Task04/%E6%9C%BA%E5%99%A8%E7%BF%BB%E8%AF%91%E5%8F%8A%E7%9B%B8%E5%85%B3%E6%8A%80%E6%9C%AF/04.png)

预测

![img](https://github.com/makeittrue/dssdxx-learning-note/blob/master/images/Task04/%E6%9C%BA%E5%99%A8%E7%BF%BB%E8%AF%91%E5%8F%8A%E7%9B%B8%E5%85%B3%E6%8A%80%E6%9C%AF/05.png)

**具体结构：**

![img](https://github.com/makeittrue/dssdxx-learning-note/blob/master/images/Task04/%E6%9C%BA%E5%99%A8%E7%BF%BB%E8%AF%91%E5%8F%8A%E7%9B%B8%E5%85%B3%E6%8A%80%E6%9C%AF/06.png)

**Encoder**

In [14]:

    class Seq2SeqEncoder(d2l.Encoder):    
        def __init__(self, vocab_size, embed_size, num_hiddens, num_layers,                 
                    dropout=0, **kwargs):        
            super(Seq2SeqEncoder, self).__init__(**kwargs)        
            self.num_hiddens=num_hiddens        
            self.num_layers=num_layers        
            self.embedding = nn.Embedding(vocab_size, embed_size)        
            self.rnn = nn.LSTM(embed_size,num_hiddens, num_layers, dropout=dropout)       
                    
        def begin_state(self, batch_size, device):        
            return [torch.zeros(size=(self.num_layers, batch_size, self.num_hiddens),  device=device),                
            torch.zeros(size=(self.num_layers, batch_size, self.num_hiddens),  device=device)]    
        def forward(self, X, *args):        
            X = self.embedding(X) # X shape: (batch_size, seq_len, embed_size)        
            X = X.transpose(0, 1)  # RNN needs first axes to be time        
            # state = self.begin_state(X.shape[1], device=X.device)        
            out, state = self.rnn(X)        
            # The shape of out is (seq_len, batch_size, num_hiddens).        
            # state contains the hidden state and the memory cell        
            # of the last time step, the shape is (num_layers, batch_size, num_hiddens)        
            return out, state

In [15]:

    encoder = Seq2SeqEncoder(vocab_size=10, embed_size=8,num_hiddens=16, num_layers=2) 
    X = torch.zeros((4, 7),dtype=torch.long) 
    output, state = encoder(X) 
    output.shape, len(state), state[0].shape, state[1].shape

Out[15]:

    (torch.Size([7, 4, 16]), 2, torch.Size([2, 4, 16]), torch.Size([2, 4, 16]))

**Decoder**

In [16]:

    class Seq2SeqDecoder(d2l.Decoder):    
        def __init__(self, vocab_size, embed_size, num_hiddens, num_layers,                 
                    dropout=0, **kwargs):        
            super(Seq2SeqDecoder, self).__init__(**kwargs)        
            self.embedding = nn.Embedding(vocab_size, embed_size)        
            self.rnn = nn.LSTM(embed_size,num_hiddens, num_layers, dropout=dropout)        
            self.dense = nn.Linear(num_hiddens,vocab_size) 

        def init_state(self, enc_outputs, *args):        
            return enc_outputs[1]     

        def forward(self, X, state):        
            X = self.embedding(X).transpose(0, 1)        
            out, state = self.rnn(X, state)        
            # Make the batch to be the first dimension to simplify loss computation.        
            out = self.dense(out).transpose(0, 1)        
            return out, state

In [17]:

    decoder = Seq2SeqDecoder(vocab_size=10, embed_size=8,num_hiddens=16, num_layers=2) 
    state = decoder.init_state(encoder(X)) 
    out, state = decoder(X, state) 
    out.shape, len(state), state[0].shape, state[1].shape

Out[17]:

    (torch.Size([4, 7, 10]), 2, torch.Size([2, 4, 16]), torch.Size([2, 4, 16]))

**损失函数**

In [18]:

    def SequenceMask(X, X_len,value=0):    
        maxlen = X.size(1)    
        mask = torch.arange(maxlen)[None, :].to(X_len.device) < X_len[:, None]       
        X[~mask]=value    
        return X

In [19]:

    X = torch.tensor([[1,2,3], [4,5,6]]) 
    SequenceMask(X,torch.tensor([1,2]))

Out[19]:

    tensor([[1, 0, 0],        
        [4, 5, 0]])

In [20]:

    X = torch.ones((2,3, 4)) 
    SequenceMask(X, torch.tensor([1,2]),value=-1)

Out[20]:

tensor([[[ 1.,  1.,  1.,  1.],         
    [-1., -1., -1., -1.],         
    [-1., -1., -1., -1.]],         
    
    [[ 1.,  1.,  1.,  1.],         
    [ 1.,  1.,  1.,  1.],         
    [-1., -1., -1., -1.]]])

In [21]:

    class MaskedSoftmaxCELoss(nn.CrossEntropyLoss):    
        # pred shape: (batch_size, seq_len, vocab_size)    
        # label shape: (batch_size, seq_len)    
        # valid_length shape: (batch_size, )    
        def forward(self, pred, label, valid_length):        
            # the sample weights shape should be (batch_size, seq_len)        
            weights = torch.ones_like(label)       
            weights = SequenceMask(weights, valid_length).float()        
            self.reduction='none'        
            output=super(MaskedSoftmaxCELoss, self).forward(pred.transpose(1,2), label)        
            return (output*weights).mean(dim=1)

In [22]:

    loss = MaskedSoftmaxCELoss() 
    loss(torch.ones((3, 4, 10)), torch.ones((3,4),dtype=torch.long), torch.tensor([4,3,0]))

Out[22]:

    tensor([2.3026, 1.7269, 0.0000])

**训练**

In [23]:

def train_ch7(model, data_iter, lr, num_epochs, device):  # Saved in d2l    
    model.to(device)    
    optimizer = optim.Adam(model.parameters(), lr=lr)    
    loss = MaskedSoftmaxCELoss()    
    tic = time.time()    
    for epoch in range(1, num_epochs+1):        
        l_sum, num_tokens_sum = 0.0, 0.0        
        for batch in data_iter:            
            optimizer.zero_grad()            
            X, X_vlen, Y, Y_vlen = [x.to(device) for x in batch]            
            Y_input, Y_label, Y_vlen = Y[:,:-1], Y[:,1:], Y_vlen-1                        
            
            Y_hat, _ = model(X, Y_input, X_vlen, Y_vlen)            
            l = loss(Y_hat, Y_label, Y_vlen).sum()            
            l.backward()             
            
            with torch.no_grad():                
                d2l.grad_clipping_nn(model, 5, device)            
            num_tokens = Y_vlen.sum().item()            
            optimizer.step()            
            l_sum += l.sum().item()            
            num_tokens_sum += num_tokens        
        if epoch % 50 == 0:            
            print("epoch {0:4d},loss {1:.3f}, time {2:.1f} sec".format(                   
                epoch, (l_sum/num_tokens_sum), time.time()-tic))            
            tic = time.time()

In [24]:

    embed_size, num_hiddens, num_layers, dropout = 32, 32, 2, 0.0 
    batch_size, num_examples, max_len = 64, 1e3, 10 
    lr, num_epochs, ctx = 0.005, 300, d2l.try_gpu() 
    src_vocab, tgt_vocab, train_iter = d2l.load_data_nmt(    
        batch_size, max_len,num_examples) 
    encoder = Seq2SeqEncoder(    
        len(src_vocab), embed_size, num_hiddens, num_layers, dropout) 
    decoder = Seq2SeqDecoder(    
        len(tgt_vocab), embed_size, num_hiddens, num_layers, dropout) 
    model = d2l.EncoderDecoder(encoder, decoder)
    train_ch7(model, train_iter, lr, num_epochs, ctx)

epoch   50,loss 0.093, time 38.2 sec <br>
epoch  100,loss 0.046, time 37.9 sec <br>
epoch  150,loss 0.032, time 36.8 sec <br>
epoch  200,loss 0.027, time 37.5 sec <br>
epoch  250,loss 0.026, time 37.8 sec <br>
epoch  300,loss 0.025, time 37.3 sec

**测试**

In [25]:

    def translate_ch7(model, src_sentence, src_vocab, tgt_vocab, max_len, device):    
        src_tokens = src_vocab[src_sentence.lower().split(' ')]    
        src_len = len(src_tokens)    
        if src_len < max_len:        
            src_tokens += [src_vocab.pad] * (max_len - src_len)    
        enc_X = torch.tensor(src_tokens, device=device)    
        enc_valid_length = torch.tensor([src_len], device=device)    
        # use expand_dim to add the batch_size dimension.    
        enc_outputs = model.encoder(enc_X.unsqueeze(dim=0), enc_valid_length)    
        dec_state = model.decoder.init_state(enc_outputs, enc_valid_length)    
        dec_X = torch.tensor([tgt_vocab.bos], device=device).unsqueeze(dim=0)    
        predict_tokens = []    
        for _ in range(max_len):        
            Y, dec_state = model.decoder(dec_X, dec_state)        
            # The token with highest score is used as the next time step input.        
            dec_X = Y.argmax(dim=2)        
            py = dec_X.squeeze(dim=0).int().item()        
            if py == tgt_vocab.eos:            
                break        
            predict_tokens.append(py)    
        return ' '.join(tgt_vocab.to_tokens(predict_tokens))

In [26]:

    for sentence in ['Go .', 'Wow !', "I'm OK .", 'I won !']:    
        print(sentence + ' => ' + translate_ch7(        
            model, sentence, src_vocab, tgt_vocab, max_len, ctx))

Go . => va ! <br>
Wow ! => <unk> ! <br>
I'm OK . => ça va . <br>
I won ! => j'ai gagné !

**Beam Search**

简单greedy search：

![img](https://github.com/makeittrue/dssdxx-learning-note/blob/master/images/Task04/%E6%9C%BA%E5%99%A8%E7%BF%BB%E8%AF%91%E5%8F%8A%E7%9B%B8%E5%85%B3%E6%8A%80%E6%9C%AF/07.png)

维特比算法：选择整体分数最高的句子（搜索空间太大） 集束搜索：

![img](https://github.com/makeittrue/dssdxx-learning-note/blob/master/images/Task04/%E6%9C%BA%E5%99%A8%E7%BF%BB%E8%AF%91%E5%8F%8A%E7%9B%B8%E5%85%B3%E6%8A%80%E6%9C%AF/08.png)

In [ ]:


## 评论区好的问题

机器翻译任务代码总结如下

数据预处理

1. 读取数据，处理数据中的编码问题，并将无效的字符串删除

2. 分词，分词的目的就是将字符串转换成单词组成的列表。目前有很多现成的分词工具可以直接使用，也可以直接按照空格进行分词(不推荐，因为分词不是很准确)

3. 建立词典，将单词组成的列表编程单词id组成的列表，这里会得到如下几样东西

   1. 去重后词典，及其中单词对应的索引列表

   2. 还可以得到给定索引找到其对应的单词的列表，以及给定单词得到对应索引的字典。

   3. 原始语料所有词对应的词典索引的列表

4. 对数据进行padding操作。因为机器翻译模型本质上是一个固定输入长度的Seq2Sqe模型，所以我们需要设置最大的数据长度，如果超出了设定的长度直接把后面的截断，少了的，根据需要进行不同的padding

5. 制作数据生成器，但是需要注意的是对于翻译任务的数据格式，机器翻译的输入是一段文本序列，输出也是一段文本序列。



Seq2Seq模型的构建

1. Seq2Seq模型由很多钟，但是整体框架都是基于先编码后解码的框架。也就是先对输入序列使用循环神经网络对他进行编码，编码成一个向量之后，再将编码得到的向量作为一个新的解码循环神经网络的隐藏状态的输入，进行解码，一次输出一个序列的元素，再将模型训练输出的序列元素与真实标签计算损失进行学习。

2. 词嵌入，一般情况下输入到编码网络中的数据不是一个onehot向量而是经过了编码之后的向量，比如由word2vec技术，让编码后的向量由更加丰富的含义。

3. 在进行编码和解码的过程中数据都是以时间步展开，也就是(Seq_len,)这种形式的数据进行处理的

4. 对于编码与解码的循环神经网络，可以通过控制隐藏层的层数及每一层隐藏层神经元的数量来控制模型的复杂度

5. 编码部分，RNN的用0初始化隐含状态，最后的输出主要是隐藏状态,编码RNN输出的隐含状态认为是其对应的编码向量

6. 解码器的整体形状与编码器是一样的，只不过解码器的模型的隐藏状态是由编码器的输出的隐藏状态初始化的。



损失函数

1. 解码器的输出是一个和词典维度相同的向量，其每个值对应与向量索引位置对应词的分数，一般是选择分数最大的那个词作为最终的输出。

2. 在计算损失函数之前，要把padding去掉，因为padding的部分不参与计算



测试

1. 解码器在测试的时候需要将模型的输出作为下一个时间步的输入

2. Beam Search搜索算法。

   1. 假设预测的时候词典的大小为3，内容为a,b,c. beam size为2，解码的时候过程如下

   2. 生成第一个词的时候，选择概率最大的两个词，假设为a,c.那么当前的两个序列就是a和c。

   3. 生成第二个词的时候，将当前序列a和c，分别与此表中的所有词进行组合，得到新的6个序列aa ab ac ca cb cc,计算每个序列的得分，并选择得分最高的2个序列，作为新的当前序列，假如为aa cb 

   4. 后面不断重复这个过程，直到遇到结束符或者达到最大长度为止，最终输出得分最高的2个序列。
>源自https://www.boyuai.com/elites/course/cZu18YmweLv10OeV/video/MZn2AdNyGzumTprVPwV3y#comment-q5wQCAfXVm-5Zkj9w3Im0
补充：
数据预处理第四点，不对。基于rnn的Seq2Seq模型，可以处理任意长度，基于rnn的机器翻译也并不需要固定长度，关于这个你可以了解下“变长数据的读入”。
要padding，是因为像tf、pytorch这些框架要求一个batch的数据必须长度相等，不然会报错；
要截断，设置最大的数据长度是因为decode的时候达到这个长度我们就停止；再一个原因就是为了加快计算，不然的话为了单个特别长的数据，batch中的其他数据都补成这么长，多慢啊
>源自https://www.boyuai.com/elites/course/cZu18YmweLv10OeV/video/MZn2AdNyGzumTprVPwV3y#comment-NITWBUx6XmKY1nR9n6dTq
## 课后习题
1.
数据预处理中分词(Tokenization)的工作是？
把词语、标点用空格分开
√把字符形式的句子转化为单词组成的列表
把句子转化为单词ID组成的列表
去除句子中的不间断空白符等特殊字符
答案解释
选项二：正确，参考数据预处理分词(Tokenization)部分。
2.
不属于数据预处理工作的是？
得到数据生成器
建立词典
分词
√把单词转化为词向量
答案解释
选项四：错误，单词转化为词向量是模型结构的一部分，词向量层一般作为网络的第一层。
3.
下列不属于单词表里的特殊符号的是？
未知单词
√空格符
句子开始符
句子结束符
答案解释
选项二：错误，参考建立词典部分代码，空格不被认为是特殊字符，在该项目里，空格在预处理时被去除。
4.
关于集束搜索(Beam Search)说法错误的是
集束搜索结合了greedy search和维特比算法。
集束搜索使用beam size参数来限制在每一步保留下来的可能性词的数量。
集束搜索是一种贪心算法。
√集束搜索得到的是全局最优解。
答案解释
选项一：正确，参考视频末尾Beam Search。
选项二：正确，参考视频末尾Beam Search。
选项三：正确，集束搜索是维特比算法的贪心形式。
选项四：错误，集束搜索是贪心算法。
5.
不属于Encoder-Decoder应用的是
机器翻译
对话机器人
√文本分类任务
语音识别任务
答案解释
Encoder-Decoder常应用于输入序列和输出序列的长度是可变的，如选项一二四，而分类问题的输出是固定的类别，不需要使用Encoder-Decoder
6.
关于Sequence to Sequence模型说法错误的是：
√训练时decoder每个单元输出得到的单词作为下一个单元的输入单词。
预测时decoder每个单元输出得到的单词作为下一个单元的输入单词。
预测时decoder单元输出为句子结束符时跳出循环。
每个batch训练时encoder和decoder都有固定长度的输入。
答案解释
选项一：错误，参考Sequence to Sequence训练图示。
选项二：正确，参考Sequence to Sequence预测图示。
选项三：正确，参考Sequence to Sequence预测图示。
选项四：正确，每个batch的输入需要形状一致。 