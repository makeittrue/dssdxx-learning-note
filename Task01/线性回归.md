    # 线性回归

主要内容包括：

1. 线性回归的基本要素
2. 线性回归模型从零开始的实现
3. 线性回归模型使用pytorch的简洁实现

**线性回归的基本要素**

**模型**

为了简单起见，这里我们假设价格只取决于房屋状况的两个因素，即面积（平方米）和房龄（年）。接下来我们希望探索价格与这两个因素的具体关系。线性回归假设输出与各个输入之间是线性关系:

<center>price=warea⋅area+wage⋅age+b</center>

**数据集**

我们通常收集一系列的真实数据，例如多栋房屋的真实售出价格和它们对应的面积和房龄。我们希望在这个数据上面寻找模型参数来使模型的预测价格与真实价格的误差最小。在机器学习术语里，该数据集被称为训练数据集（training data set）或训练集（training set），一栋房屋被称为一个样本（sample），其真实售出价格叫作标签（label），用来预测标签的两个因素叫作特征（feature）。特征用来表征样本的特点。

**损失函数**

在模型训练中，我们需要衡量价格预测值与真实值之间的误差。通常我们会选取一个非负数作为误差，且数值越小表示误差越小。一个常用的选择是平方函数。 它在评估索引为 i 的样本误差的表达式为

<center>l(i)(w,b)=12(y^(i)−y(i))2,

L(w,b)=1n∑i=1nl(i)(w,b)=1n∑i=1n12(w⊤x(i)+b−y(i))2.</center>

**优化函数 - 随机梯度下降**

当模型和损失函数形式较为简单时，上面的误差最小化问题的解可以直接用公式表达出来。这类解叫作解析解（analytical solution）。本节使用的线性回归和平方误差刚好属于这个范畴。然而，大多数深度学习模型并没有解析解，只能通过优化算法有限次迭代模型参数来尽可能降低损失函数的值。这类解叫作数值解（numerical solution）。

在求数值解的优化算法中，小批量随机梯度下降（mini-batch stochastic gradient descent）在深度学习中被广泛使用。它的算法很简单：先选取一组模型参数的初始值，如随机选取；接下来对参数进行多次迭代，使每次迭代都可能降低损失函数的值。在每次迭代中，先随机均匀采样一个由固定数目训练数据样本所组成的小批量（mini-batch）B，然后求小批量中数据样本的平均损失有关模型参数的导数（梯度），最后用此结果与预先设定的一个正数的乘积作为模型参数在本次迭代的减小量。

<center>(w,b)←(w,b)−η|B|∑i∈B∂(w,b)l(i)(w,b)</center>

学习率: η代表在每次优化中，能够学习的步长的大小

批量大小: B是小批量计算中的批量大小batch size

总结一下，优化函数的有以下两个步骤：

- (i)初始化模型参数，一般来说使用随机初始化；
- (ii)我们在数据上迭代多次，通过在负梯度方向移动参数来更新每个参数。

**矢量计算**

在模型训练或预测时，我们常常会同时处理多个数据样本并用到矢量计算。在介绍线性回归的矢量计算表达式之前，让我们先考虑对两个向量相加的两种方法。

1. 向量相加的一种方法是，将这两个向量按元素逐一做标量加法。
2. 向量相加的另一种方法是，将这两个向量直接做矢量加法。

In [1]:

    import torch 
    import time 
    # init variable a, b as 1000 dimension vector 
    n = 1000 
    a = torch.ones(n) 
    b = torch.ones(n)

In [2]:

    # define a timer class to record time 
    class Timer(object):    
        """Record multiple running times."""    
        def __init__(self):        
            self.times = []        
            self.start()     
        def start(self):        
            # start the timer        
            self.start_time = time.time()     
        def stop(self):        
            # stop the timer and record time into a list        
            self.times.append(time.time() - self.start_time)        
            return self.times[-1]     
        def avg(self):        
            # calculate the average and return        
            return sum(self.times)/len(self.times)     
        def sum(self):        
            # return the sum of recorded time        
            return sum(self.times)

现在我们可以来测试了。首先将两个向量使用for循环按元素逐一做标量加法。

In [3]:

    timer = Timer() 
    c = torch.zeros(n) 
    for i in range(n):    
        c[i] = a[i] + b[i] 
    '%.5f sec' % timer.stop()

另外是使用torch来将两个向量直接做矢量加法：

In [4]:

    timer.start() 
    d = a + b 
    '%.5f sec' % timer.stop()

结果很明显,后者比前者运算速度更快。因此，我们应该尽可能采用矢量计算，以提升计算效率。

**线性回归模型从零开始的实现**

In [5]:

    # import packages and modules 
    %matplotlib inline 
    import torch from IPython 
    import display from matplotlib 
    import pyplot as plt 
    import numpy as np 
    import random print(torch.__version__)

**生成数据集**

使用线性模型来生成数据集，生成一个1000个样本的数据集，下面是用来生成数据的线性关系：

<center>price=warea⋅area+wage⋅age+b</center>

In [6]:

    # set input feature number  
    num_inputs = 2 
    # set example number 
    num_examples = 1000 
    # set true weight and bias in order to generate corresponded label 
    true_w = [2, -3.4] 
    true_b = 4.2 

    features = torch.randn(num_examples, num_inputs,                      
    dtype=torch.float32) 
    labels = true_w[0] * features[:, 0] + true_w[1] * features[:, 1] + true_b 
    labels += torch.tensor(np.random.normal(0, 0.01, size=labels.size()),                       dtype=torch.float32) #加上一个偏差 通过正态分布随机生成

**使用图像来展示生成的数据**

In [7]:

    plt.scatter(features[:, 1].numpy(), labels.numpy(), 1) #生成散点图

**读取数据集**

In [8]:

    def data_iter(batch_size, features, labels):    
        num_examples = len(features)    
        indices = list(range(num_examples))    
        random.shuffle(indices)  # random read 10 samples    
        for i in range(0, num_examples, batch_size):        
            j = torch.LongTensor(indices[i: min(i + batch_size, num_examples)]) # the last time may be not enough for a whole batch        
            yield  features.index_select(0, j), labels.index_select(0, j) 

In [9]: 
    batch_size = 10 for X, y in data_iter(batch_size, features, labels):    
        print(X, '\n', y)    
        break

**初始化模型参数**

In [10]:

    w = torch.tensor(np.random.normal(0, 0.01, (num_inputs, 1)), dtype=torch.float32) 
    b = torch.zeros(1, dtype=torch.float32) 

    w.requires_grad_(requires_grad=True) #附加梯度 
    b.requires_grad_(requires_grad=True)

**定义模型**

定义用来训练参数的训练模型：

<center>price=warea⋅area+wage⋅age+b</center>

In [11]:

    def linreg(X, w, b):    
        return torch.mm(X, w) + b #权重和特征相乘 torch.mm为矩阵相乘

**定义损失函数**

我们使用的是均方误差损失函数：

<center>l(i)(w,b)=1/2(y^(i)−y(i))^2,</center>

In [12]:

def squared_loss(y_hat, y):     
    return (y_hat - y.view(y_hat.size())) ** 2 / 2

**定义优化函数**

在这里优化函数使用的是小批量随机梯度下降：

<center>(w,b)←(w,b)−η|B|∑i∈B∂(w,b)l(i)(w,b)</center>

In [13]:

    def sgd(params, lr, batch_size):     
        for param in params:        
        param.data -= lr * param.grad / batch_size # ues .data to operate param without gradient track

**训练**

当数据集、模型、损失函数和优化函数定义完了之后就可来准备进行模型的训练了。

In [14]:

    # super parameters init 
    lr = 0.03 
    num_epochs = 5 

    net = linreg #单层线性网络 
    loss = squared_loss 
    # training 
    for epoch in range(num_epochs):  # training repeats num_epochs times    
        # in each epoch, all the samples in dataset will be used once        
        # X is the feature and y is the label of a batch sample    
        for X, y in data_iter(batch_size, features, labels):        
        l = loss(net(X, w, b), y).sum()          
        # calculate the gradient of batch sample loss         
        l.backward()          
        # using small batch random gradient descent to iter model parameters        
        sgd([w, b], lr, batch_size)          
        # reset parameter gradient        
        w.grad.data.zero_() #参数清零        
        b.grad.data.zero_()    
    train_l = loss(net(features, w, b), labels)    
    print('epoch %d, loss %f' % (epoch + 1, train_l.mean().item()))

In [15]:

    w, true_w, b, true_b

**线性回归模型使用pytorch的简洁实现**

In [16]:

    import torch from torch 
    import nn 
    import numpy as np torch.manual_seed(1) 

    print(torch.__version__) 
    torch.set_default_tensor_type('torch.FloatTensor')

**生成数据集**

在这里生成数据集跟从零开始的实现中是完全一样的。

In [17]:

    num_inputs = 2 
    num_examples = 1000 

    true_w = [2, -3.4] 
    true_b = 4.2 

    features = torch.tensor(np.random.normal(0, 1, (num_examples, num_inputs)), dtype=torch.float) 
    labels = true_w[0] * features[:, 0] + true_w[1] * features[:, 1] + true_b 
    labels += torch.tensor(np.random.normal(0, 0.01, size=labels.size()), dtype=torch.float)

**读取数据集**

In [18]:

    import torch.utils.data as Data 

    batch_size = 10 

    # combine featues and labels of dataset 
    dataset = Data.TensorDataset(features, labels) 

    # put dataset into DataLoader 
    data_iter = Data.DataLoader(    
        dataset=dataset,            # torch TensorDataset format    batch_size=batch_size,      # mini batch size    shuffle=True,               # whether shuffle the data or not    num_workers=2,              # read data in multithreading 
        )

In [19]:

    for X, y in data_iter:    
        print(X, '\n', y)    
        break

**定义模型**

In [20]:

    class LinearNet(nn.Module):    
        def __init__(self, n_feature):        
            super(LinearNet, self).__init__()      # call father function to init         
            self.linear = nn.Linear(n_feature, 1)  # function prototype: `torch.nn.Linear(in_features, out_features, bias=True)`     
        def forward(self, x):        
            y = self.linear(x)        
            return y     
        net = LinearNet(num_inputs) 
        print(net) 

In [21]:

    # ways to init a multilayer network 
    # method one 
    net = nn.Sequential(    
        nn.Linear(num_inputs, 1)    
        # other layers can be added here    
        )

    # method two 
    net = nn.Sequential() 
    net.add_module('linear', nn.Linear(num_inputs, 1)) 
    # net.add_module ...... 

    # method three 
    from collections import OrderedDict 
    net = nn.Sequential(OrderedDict([          
        ('linear', nn.Linear(num_inputs, 1))          
        # ......        
        ])) 
    print(net) 
    print(net[0])

**初始化模型参数**

In [22]:

    from torch.nn import init

    init.normal_(net[0].weight, mean=0.0, std=0.01) 
    init.constant_(net[0].bias, val=0.0)  # or you can use `net[0].bias.data.fill_(0)` to modify it directly

In [23]:

    for param in net.parameters():    
        print(param)

**定义损失函数**

In [24]:

    loss = nn.MSELoss()    # nn built-in squared loss function                       
    # function prototype: `torch.nn.MSELoss(size_average=None, reduce=None, reduction='mean')`

**定义优化函数**

In [25]:

    import torch.optim as optim 

    optimizer = optim.SGD(net.parameters(), lr=0.03)   # built-in random gradient descent function 
    print(optimizer)  # function prototype: torch.optim.SGD(params, lr=, momentum=0, dampening=0, weight_decay=0, nesterov=False)

**训练**

In [26]:

num_epochs = 3 
for epoch in range(1, num_epochs + 1):    
    for X, y in data_iter:        
        output = net(X)        
        l = loss(output, y.view(-1, 1))        
        optimizer.zero_grad() # reset gradient, equal to net.zero_grad()        
        l.backward()        
        optimizer.step()    
    print('epoch %d, loss: %f' % (epoch, l.item()))

In [27]:

    # result comparision 
    dense = net[0] 
    print(true_w, dense.weight.data) 
    print(true_b, dense.bias.data)

**两种实现方式的比较**

1. 从零开始的实现（推荐用来学习）

能够更好的理解模型和神经网络底层的原理

1. 使用pytorch的简洁实现

能够更加快速地完成模型的设计与实现

选择题

1.

假如你正在实现一个全连接层，全连接层的输入形状是7×8，输出形状是7×1，其中7是批量大小，则权重参数*w*和偏置参数*b*的形状分别是____和____

1×8，1×1

1×8，7×1

8×1，1×1

8×1，7×1

答案解释

设输入批量为*X*∈R7×8，对应的输出为*Y*∈R7×1，令权重参数为*w*∈R8×1，则*Xw*∈R7×1，然后我们给*Xw*中的每个元素加上的偏置是一样的，所以偏置参数*b*∈R1×1，基于加法的广播机制，可以完成得到输出*Y*=*Xw*+*b*。参数的形状与批量大小没有关系，也正是因为如此，对同一个模型，我们可以选择不同的批量大小。

2.

课程中的损失函数定义为：

def squared_loss(y_hat, y): 
    return (y_hat - y.view(y_hat.size())) ** 2 / 2

将返回结果替换为下面的哪一个会导致会导致模型无法训练：（阅读材料：https://pytorch.org/docs/stable/notes/broadcasting.html）

(y_hat.view(-1) - y) ** 2 / 2

(y_hat - y.view(-1)) ** 2 / 2

(y_hat - y.view(y_hat.shape)) ** 2 / 2

(y_hat - y.view(-1, 1)) ** 2 / 2

答案解释

y_hat的形状是[n, 1]，而y的形状是[n]，两者相减得到的结果的形状是[n, n]，相当于用y_hat的每一个元素分别减去y的所有元素，所以无法得到正确的损失值。对于第一个选项，y_hat.view(-1)的形状是[n]，与y一致，可以相减；对于第二个选项，y.view(-1)的形状仍是[n]，所以没有解决问题；对于第三个选项和第四个选项，y.view(y_hat.shape)和y.view(-1, 1)的形状都是[n, 1]，与y_hat一致，可以相减。以下是一段示例代码：

x = torch.arange(3) y = torch.arange(3).view(3, 1) print(x) print(y) print(x + y)

填空题

1.

在线性回归模型中，对于某个大小为3的批量，标签的预测值和真实值如下表所示：

| *y*^ | *y*  |
| ---- | ---- |
| 2.33 | 3.14 |
| 1.07 | 0.98 |
| 1.23 | 1.32 |

该批量的损失函数的平均值为：（参考“线性回归模型从零开始的实现”中的“定义损失函数”一节，结果保留三位小数）

答案解释

批量的损失函数平均值为：*L*=2*n*1*i*=1∑*n*(*y*^*i*−*y**i*)2，其中*n*是批量大小。
>https://www.boyuai.com/elites/course/cZu18YmweLv10OeV